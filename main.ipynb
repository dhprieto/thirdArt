{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading, cleaning and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dres2\\AppData\\Local\\Temp\\ipykernel_14256\\4052692268.py:56: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  numCols = df_read.select_dtypes(include=np.number).drop(\"numVol\",1).columns\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def fullRead(pathToTable, sep, anthro = False):\n",
    "\n",
    "  df_renamed = pd.read_csv(pathToTable, sep = sep, encoding = \"latin_1\")\n",
    "\n",
    "\n",
    "  # reading and merging    \n",
    "  \n",
    "  if anthro == True:\n",
    "    df_anthro = pd.read_csv(\"data/chronicAnthropometricCardiovascularData.csv\", sep=\";\", decimal=\",\")\n",
    "    df_renamed = df_renamed.merge(df_anthro)\n",
    "\n",
    "    # separating by time moment and renaming\n",
    "\n",
    "    df_renamed[\"Weight\"] = \"\"\n",
    "    df_renamed[\"BMI\"] = \"\"\n",
    "    df_renamed[\"Fat\"] = \"\"\n",
    "    df_renamed[\"CVRI\"] = \"\"\n",
    "    df_renamed[\"Bpmin\"] = \"\"\n",
    "    df_renamed[\"Bpmax\"] = \"\"\n",
    "    df_renamed[\"Frec\"] = \"\"\n",
    "\n",
    "    for i in range(len(df_renamed)):\n",
    "\n",
    "        if df_renamed.loc[i][\"Time\"] == \"Initial\":\n",
    "            df_renamed.loc[i,\"Weight\"] = df_renamed.loc[i][\"Peso inicial\"]\n",
    "            df_renamed.loc[i,\"BMI\"] = df_renamed.loc[i][\"IMC Inicial\"]\n",
    "            df_renamed.loc[i,\"Fat\"] = df_renamed.loc[i][\"Grasa inicial\"]\n",
    "            df_renamed.loc[i,\"CVRI\"] = df_renamed.loc[i][\"IRCV inicial\"] \n",
    "            df_renamed.loc[i,\"Bpmin\"] = df_renamed.loc[i][\"Bpmin inicial\"] \n",
    "            df_renamed.loc[i,\"Bpmax\"] = df_renamed.loc[i][\"Bpmax inicial\"] \n",
    "            df_renamed.loc[i,\"Frec\"] = df_renamed.loc[i][\"Frec inicial\"] \n",
    "                \n",
    "        if df_renamed.loc[i][\"Time\"] == \"Final\":\n",
    "        \n",
    "            df_renamed.loc[i,\"Weight\"] = df_renamed.loc[i][\"Peso final\"]\n",
    "            df_renamed.loc[i,\"BMI\"] = df_renamed.loc[i][\"IMC Final\"]\n",
    "            df_renamed.loc[i,\"Fat\"] = df_renamed.loc[i][\"Grasa final\"]\n",
    "            df_renamed.loc[i,\"CVRI\"] = df_renamed.loc[i][\"IRCV Final\"] \n",
    "            df_renamed.loc[i,\"Bpmin\"] = df_renamed.loc[i][\"Bpmin final\"] \n",
    "            df_renamed.loc[i,\"Bpmax\"] = df_renamed.loc[i][\"Bpmax final\"] \n",
    "            df_renamed.loc[i,\"Frec\"] = df_renamed.loc[i][\"Frec final\"] \n",
    "        \n",
    "    df_renamed.drop(columns = [\"Peso inicial\", \"Peso final\", \"Delta Peso\", \"Talla\", \"IMC Inicial\", \"IMC Final\", \"Delta IMC\", \"Grasa inicial\", \"Grasa final\", \"Delta Grasa\", \"IRCV Final\", \"IRCV inicial\", \"Bpmin final\", \"Bpmin inicial\", \"Bpmax final\", \"Bpmax inicial\", \"Frec final\", \"Frec inicial\",], inplace=True )\n",
    "  \n",
    "  df_renamed.drop(columns = [\"Unnamed: 0\", \"grouping\"], inplace=True )\n",
    "  df_renamed.fillna(0, inplace=True)\n",
    "  return df_renamed\n",
    "\n",
    "def scaling(df_read):\n",
    "   \n",
    "   scaler = preprocessing.MinMaxScaler()\n",
    "   numCols = df_read.select_dtypes(include=np.number).drop(\"numVol\",1).columns\n",
    "   df_read[numCols] = scaler.fit_transform(df_read[numCols])\n",
    "   return df_read\n",
    "\n",
    "df = scaling(fullRead(\"data/plasmAnt_ord.csv\",  sep = \",\", anthro= True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split and encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    " \n",
    "enc = OrdinalEncoder()\n",
    "enc.fit(df[[\"Sweetener\", \"Sex\", \"Time\"]])\n",
    "df[[\"Sweetener\", \"Sex\", \"Time\"]] = enc.transform(df[[\"Sweetener\", \"Sex\",  \"Time\"]])\n",
    "\n",
    "# Initial = 1, Final = 0\n",
    "\n",
    "X, y = df[df[\"Time\"] == 1].drop([\"numVol\", \"Time\"], axis=1), df[df[\"Time\"] == 0].drop(['Time','numVol', 'Sweetener', 'Time', 'Sex', 'Weight','BMI', 'Fat', 'CVRI', 'Bpmin', 'Bpmax', 'Frec'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_full, y_full = df[df[\"Time\"] == 1].drop([\"numVol\", \"Time\"], axis=1), df[df[\"Time\"] == 0].drop(['Time','numVol', 'Sweetener', 'Time'], axis = 1)\n",
    "X_fulltrain, X_fulltest, y_fulltrain, y_fulltest = train_test_split(X_full, y_full, test_size=0.3, random_state=42)\n",
    "\n",
    "#X_test.to_csv(\"X_met_test_urineAnt.csv\", index=False)\n",
    "#X_fulltest.to_csv(\"X_full_test_urineAnt.csv\",index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF - metabolics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n",
      "MAE: 0.119 (0.010)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "# define model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = model, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_grid = grid_search.best_estimator_\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model and collect the scores\n",
    "n_scores = cross_val_score(best_grid, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# force the scores to be positive\n",
    "n_scores = np.absolute(n_scores)\n",
    "# summarize performance\n",
    "\n",
    "#filename = 'rf_met_plasmAnt.pkl'\n",
    "#with open(filename, 'wb') as file:\n",
    "#    pickle.dump(grid_search.best_estimator_, file)\n",
    "\n",
    "print('MAE: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF FULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n",
      "MAE: 0.116 (0.008)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "\n",
    "# define model\n",
    "model = RandomForestRegressor()\n",
    "\n",
    "grid_search = GridSearchCV(estimator = model, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "\n",
    "grid_search.fit(X_fulltrain, y_fulltrain)\n",
    "best_grid = grid_search.best_estimator_\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model and collect the scores\n",
    "n_scores = cross_val_score(best_grid, X_full, y_full, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# force the scores to be positive\n",
    "n_scores = np.absolute(n_scores)\n",
    "\n",
    "#filename = 'rf_full_plasmAnt.pkl'\n",
    "#with open(filename, 'wb') as file:\n",
    "#    pickle.dump(grid_search.best_estimator_, file)\n",
    "\n",
    "# summarize performance\n",
    "print('MAE: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mlp for multi-output regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Full parameters gridsearch\n",
    "# DONT EXECUTE\n",
    "'''\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "weight_constraint = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs, optimizer=optimizer, optimizer__learning_rate=learn_rate, optimizer__momentum=momentum,\n",
    "                  model__init_mode=init_mode, model__activation=activation, model__dropout_rate=dropout_rate, model__weight_constraint=weight_constraint)\n",
    "\n",
    "# get the model\n",
    "def get_model(n_inputs, n_outputs, dropout_rate, weight_constraint, activation=\"relu\", init_mode='uniform'):\n",
    "    model_nn = Sequential()\n",
    "    model_nn.add(Dense(64, input_shape=(X.shape[1],),activation=activation, kernel_initializer=init_mode, kernel_constraint=MaxNorm(weight_constraint)))\n",
    "    model_nn.add(Dropout(dropout_rate))\n",
    "    model_nn.add(Dense(32, activation=activation, kernel_initializer=init_mode))\n",
    "    model_nn.add(Dropout(dropout_rate))\n",
    "    model_nn.add(Dense(y.shape[1], activation='linear', kernel_initializer=init_mode))\n",
    "    return model_nn\n",
    " \n",
    "# evaluate a model using repeated k-fold cross-validation\n",
    "def evaluate_model(X, y):\n",
    " results = list()\n",
    " n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    " # define evaluation procedure\n",
    " cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    " X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# define modeld\n",
    " model_nn = KerasRegressor(model = get_model) #(n_inputs, n_outputs, activation=\"relu\", init_mode='uniform'))\n",
    " # fit model\n",
    " grid = GridSearchCV(estimator=model_nn, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    " grid_result = grid.fit(X_train, y_train) # evaluate model on test set\n",
    " mae = model_nn.evaluate(X_test, y_test)\n",
    " # store result\n",
    " print('>%.3f' % mae)\n",
    " results.append(mae)\n",
    " return results\n",
    " \n",
    "# evaluate model\n",
    "results = evaluate_model(X, y)\n",
    "# summarize performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(results), std(results)))\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP only metabolics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.112 (0.013)\n"
     ]
    }
   ],
   "source": [
    "# mlp for multi-output regression\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "epochs = [10, 50, 100]\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# get the model\n",
    "def get_model(n_inputs, n_outputs):\n",
    "    model_nn = Sequential()\n",
    "    model_nn.add(Dense(64, input_shape=(X.shape[1],),activation=\"relu\"))\n",
    "    model_nn.add(Dropout(0.5))\n",
    "    model_nn.add(Dense(32, activation=\"relu\"))\n",
    "    model_nn.add(Dropout(0.5))\n",
    "    model_nn.add(Dense(y.shape[1], activation='linear'))\n",
    "    model_nn.compile(loss='mae', optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "    return model_nn\n",
    " \n",
    "# evaluate a model using repeated k-fold cross-validation\n",
    "def evaluate_model(X, y):\n",
    " results = list()\n",
    " n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    " # define evaluation procedure\n",
    " cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    " X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# define modeld\n",
    " model_nn = KerasRegressor(model = get_model(n_inputs, n_outputs), optimizer=tf.keras.optimizers.Adam(), verbose=0)\n",
    " # fit model\n",
    " grid = GridSearchCV(estimator=model_nn, param_grid=param_grid, n_jobs=-1, cv=3, verbose=0)\n",
    " grid_result = grid.fit(X_train, y_train) # evaluate model on test set\n",
    " # summarize results\n",
    " best_grid = grid_result.best_estimator_\n",
    "# define the evaluation procedure\n",
    " cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model and collect the scores\n",
    " n_scores = cross_val_score(best_grid, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# force the scores to be positive\n",
    " n_scores = np.absolute(n_scores)\n",
    "\n",
    "# store the model\n",
    "# filename = 'mlp_met_plasmAnt.pkl'\n",
    "# with open(filename, 'wb') as file:\n",
    "#    pickle.dump(grid_result.best_estimator_, file)\n",
    "\n",
    "# summarize performance\n",
    " print('MAE: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "# evaluate model\n",
    "evaluate_model(X, y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.110 (0.010)\n"
     ]
    }
   ],
   "source": [
    "# mlp for multi-output regression\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "epochs = [10, 50, 100]\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "# get the model\n",
    "def get_model(n_inputs, n_outputs):\n",
    "    model_nn = Sequential()\n",
    "    model_nn.add(Dense(64, input_shape=(X.shape[1],),activation=\"relu\"))\n",
    "    model_nn.add(Dropout(0.5))\n",
    "    model_nn.add(Dense(32, activation=\"relu\"))\n",
    "    model_nn.add(Dropout(0.5))\n",
    "    model_nn.add(Dense(y_full.shape[1], activation='linear'))\n",
    "    model_nn.compile(loss='mae', optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "    return model_nn\n",
    " \n",
    "def evaluate_model(X, y):\n",
    " results = list()\n",
    " n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    " # define evaluation procedure\n",
    " cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    " X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# define modeld\n",
    " model_nn = KerasRegressor(model = get_model(n_inputs, n_outputs), optimizer=tf.keras.optimizers.Adam(), verbose=0)\n",
    " # fit model\n",
    " grid = GridSearchCV(estimator=model_nn, param_grid=param_grid, n_jobs=-1, cv=3, verbose=0)\n",
    " grid_result = grid.fit(X_train, y_train) # evaluate model on test set\n",
    " # summarize results\n",
    " best_grid = grid_result.best_estimator_\n",
    "# define the evaluation procedure\n",
    " cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# evaluate the model and collect the scores\n",
    " n_scores = cross_val_score(best_grid, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# force the scores to be positive\n",
    " n_scores = np.absolute(n_scores)\n",
    "\n",
    "# store the model\n",
    "# filename = 'mlp_full_plasmAnt.pkl'\n",
    "# with open(filename, 'wb') as file:\n",
    "#    pickle.dump(grid_result.best_estimator_, file)\n",
    "\n",
    "# summarize performance\n",
    " print('MAE: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n",
    "\n",
    "# evaluate model\n",
    "evaluate_model(X_full, y_full)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
