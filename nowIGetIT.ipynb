{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dres2\\AppData\\Local\\Temp\\ipykernel_13960\\2456273508.py:56: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  numCols = df_read.select_dtypes(include=np.number).drop(\"numVol\",1).columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['numVol', 'CA', 'CA.G', 'CA.S', 'CA.GS', 'Total.CA', 'DHPAA', 'DHPAA.G',\n",
       "       'DHPAA.GG', 'DHPAA.GS', 'DHPAA.SS', 'Total.DHPAA', 'TFA.G', 'TFA.S',\n",
       "       'TFA.di.sulfate.1', 'Total.TFA', 'TIFA.Sulfate.1', 'VA', 'VA.GG',\n",
       "       'VA.GS', 'VA.SS', 'Total.VA', 'Sweetener', 'Time', 'Sex', 'Weight',\n",
       "       'BMI', 'Fat', 'CVRI', 'Bpmin', 'Bpmax', 'Frec'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def fullRead(pathToTable, sep, anthro = False):\n",
    "\n",
    "  df_renamed = pd.read_csv(pathToTable, sep = sep, encoding = \"latin_1\")\n",
    "\n",
    "\n",
    "  # reading and merging    \n",
    "  \n",
    "  if anthro == True:\n",
    "    df_anthro = pd.read_csv(\"data/chronicAnthropometricCardiovascularData.csv\", sep=\";\", decimal=\",\")\n",
    "    df_renamed = df_renamed.merge(df_anthro)\n",
    "\n",
    "    # separating by time moment and renaming\n",
    "\n",
    "    df_renamed[\"Weight\"] = \"\"\n",
    "    df_renamed[\"BMI\"] = \"\"\n",
    "    df_renamed[\"Fat\"] = \"\"\n",
    "    df_renamed[\"CVRI\"] = \"\"\n",
    "    df_renamed[\"Bpmin\"] = \"\"\n",
    "    df_renamed[\"Bpmax\"] = \"\"\n",
    "    df_renamed[\"Frec\"] = \"\"\n",
    "\n",
    "    for i in range(len(df_renamed)):\n",
    "\n",
    "        if df_renamed.loc[i][\"Time\"] == \"Initial\":\n",
    "            df_renamed.loc[i,\"Weight\"] = df_renamed.loc[i][\"Peso inicial\"]\n",
    "            df_renamed.loc[i,\"BMI\"] = df_renamed.loc[i][\"IMC Inicial\"]\n",
    "            df_renamed.loc[i,\"Fat\"] = df_renamed.loc[i][\"Grasa inicial\"]\n",
    "            df_renamed.loc[i,\"CVRI\"] = df_renamed.loc[i][\"IRCV inicial\"] \n",
    "            df_renamed.loc[i,\"Bpmin\"] = df_renamed.loc[i][\"Bpmin inicial\"] \n",
    "            df_renamed.loc[i,\"Bpmax\"] = df_renamed.loc[i][\"Bpmax inicial\"] \n",
    "            df_renamed.loc[i,\"Frec\"] = df_renamed.loc[i][\"Frec inicial\"] \n",
    "                \n",
    "        if df_renamed.loc[i][\"Time\"] == \"Final\":\n",
    "        \n",
    "            df_renamed.loc[i,\"Weight\"] = df_renamed.loc[i][\"Peso final\"]\n",
    "            df_renamed.loc[i,\"BMI\"] = df_renamed.loc[i][\"IMC Final\"]\n",
    "            df_renamed.loc[i,\"Fat\"] = df_renamed.loc[i][\"Grasa final\"]\n",
    "            df_renamed.loc[i,\"CVRI\"] = df_renamed.loc[i][\"IRCV Final\"] \n",
    "            df_renamed.loc[i,\"Bpmin\"] = df_renamed.loc[i][\"Bpmin final\"] \n",
    "            df_renamed.loc[i,\"Bpmax\"] = df_renamed.loc[i][\"Bpmax final\"] \n",
    "            df_renamed.loc[i,\"Frec\"] = df_renamed.loc[i][\"Frec final\"] \n",
    "        \n",
    "    df_renamed.drop(columns = [\"Peso inicial\", \"Peso final\", \"Delta Peso\", \"Talla\", \"IMC Inicial\", \"IMC Final\", \"Delta IMC\", \"Grasa inicial\", \"Grasa final\", \"Delta Grasa\", \"IRCV Final\", \"IRCV inicial\", \"Bpmin final\", \"Bpmin inicial\", \"Bpmax final\", \"Bpmax inicial\", \"Frec final\", \"Frec inicial\",], inplace=True )\n",
    "  \n",
    "  df_renamed.drop(columns = [\"Unnamed: 0\", \"grouping\"], inplace=True )\n",
    "  df_renamed.fillna(0, inplace=True)\n",
    "  return df_renamed\n",
    "\n",
    "def scaling(df_read):\n",
    "   \n",
    "   scaler = preprocessing.MinMaxScaler()\n",
    "   numCols = df_read.select_dtypes(include=np.number).drop(\"numVol\",1).columns\n",
    "   df_read[numCols] = scaler.fit_transform(df_read[numCols])\n",
    "   return df_read\n",
    "\n",
    "df1 = scaling(fullRead(\"data/urineAnt_ord.csv\",  sep = \",\", anthro= True))\n",
    "df1.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing new slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "metabs = ['CA', 'CA.G', 'CA.S', 'CA.GS', 'Total.CA', 'DHPAA', 'DHPAA.G',\n",
    "       'DHPAA.GG', 'DHPAA.GS', 'DHPAA.SS', 'Total.DHPAA', 'TFA.G', 'TFA.S',\n",
    "       'TFA.di.sulfate.1', 'Total.TFA', 'TIFA.Sulfate.1', 'VA', 'VA.GG',\n",
    "       'VA.GS', 'VA.SS', 'Total.VA']\n",
    "anthro = ['Weight','BMI', 'Fat', 'CVRI', 'Bpmin', 'Bpmax', 'Frec']\n",
    "factors = ['Sweetener', 'Time', 'Sex']\n",
    "enc = OrdinalEncoder()\n",
    "enc.fit(df1[factors])\n",
    "df1[factors] = enc.transform(df1[factors])\n",
    "\n",
    "#df_0 = df1_anthro[df1_anthro[\"Time\"] == 1]\n",
    "#df_metab0 = df_0[metabs]\n",
    "\n",
    "#df_metab0antrofull = df_metab0.join(df1_anthro[anthro + factors + [\"numVol\"]], )\n",
    "#df_metab0antrofull[\"Time\"]\n",
    "X, y = df1.drop([\"numVol\"], axis=1), df1.drop(anthro + factors + [\"numVol\"], axis =1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">0.074\n",
      "MAE: 0.074 (0.000)\n"
     ]
    }
   ],
   "source": [
    "# mlp for multi-output regression\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    " \n",
    "#enc = OrdinalEncoder()\n",
    "#enc.fit(df1[[\"Sweetener\", \"Sex\"]])\n",
    "#df1[[\"Sweetener\", \"Sex\"]] = enc.transform(df1[[\"Sweetener\", \"Sex\"]])\n",
    "\n",
    "#X, y = df1[df1[\"Time\"] == \"Initial\"].drop([\"numVol\", \"Time\"], axis=1), df1[df1[\"Time\"] == \"Final\"].drop(['Time','numVol', 'Sweetener', 'Time', 'Sex', 'Weight','BMI', 'Fat', 'CVRI', 'Bpmin', 'Bpmax', 'Frec'], axis = 1)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# get the model\n",
    "def get_model(n_inputs, n_outputs):\n",
    " model = Sequential()\n",
    " model.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
    " model.add(Dense(n_outputs))\n",
    " model.compile(loss='mae', optimizer='adam')\n",
    " return model\n",
    "\n",
    "# evaluate a model using repeated k-fold cross-validation\n",
    "def evaluate_model(X, y, X_train, X_test, y_train, y_test):\n",
    " results = list()\n",
    " n_inputs, n_outputs = X.shape[1], y.shape[1]\n",
    " # define evaluation procedure\n",
    " cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    " # enumerate folds\n",
    " #for train_ix, test_ix in cv.split(X):\n",
    "  #  print (train_ix)\n",
    "    # prepare data\n",
    "    # X_train, X_test = X[train_ix], X[test_ix]\n",
    "    # y_train, y_test = y[train_ix], y[test_ix]\n",
    "    # define model\n",
    " model = get_model(n_inputs, n_outputs)\n",
    " # fit model\n",
    " model.fit(X_train, y_train, verbose=0, epochs=500)\n",
    " # evaluate model on test set\n",
    " mae = model.evaluate(X_test, y_test, verbose=0)\n",
    " # store result\n",
    " print('>%.3f' % mae)\n",
    " results.append(mae)\n",
    " return results \n",
    "\n",
    "# evaluate model\n",
    "results = evaluate_model(X, y, X_train, X_test, y_train, y_test)\n",
    "# summarize performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(results), std(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Dense, Dropout\n\u001b[0;32m      5\u001b[0m X, y \u001b[39m=\u001b[39m df1[df1[\u001b[39m\"\u001b[39m\u001b[39mTime\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInitial\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mdrop([\u001b[39m\"\u001b[39m\u001b[39mnumVol\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mTime\u001b[39m\u001b[39m\"\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), df1[df1[\u001b[39m\"\u001b[39m\u001b[39mTime\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mFinal\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mdrop([\u001b[39m'\u001b[39m\u001b[39mTime\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mnumVol\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSweetener\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTime\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSex\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mWeight\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mBMI\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFat\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCVRI\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mBpmin\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mBpmax\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mFrec\u001b[39m\u001b[39m'\u001b[39m], axis \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Define custom feature weights\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m#weights = [0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[39m# Create the deep learning model\u001b[39;00m\n\u001b[0;32m     13\u001b[0m model \u001b[39m=\u001b[39m Sequential()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "X, y = df1[df1[\"Time\"] == \"Initial\"].drop([\"numVol\", \"Time\"], axis=1), df1[df1[\"Time\"] == \"Final\"].drop(['Time','numVol', 'Sweetener', 'Time', 'Sex', 'Weight','BMI', 'Fat', 'CVRI', 'Bpmin', 'Bpmax', 'Frec'], axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define custom feature weights\n",
    "#weights = [0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.043, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]\n",
    "\n",
    "# Create the deep learning model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(X.shape[1],), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y.shape[1], activation='linear'))\n",
    "\n",
    "# Compile the model with weighted loss function\n",
    "def weighted_loss(weights):\n",
    "    def loss(y_true, y_pred):\n",
    "        return np.sum(np.square((y_true - y_pred) * weights))\n",
    "    return loss\n",
    "\n",
    "model.compile(loss=\"mae\", optimizer='adam')\n",
    "\n",
    "# Fit the model to the training data\n",
    "model.fit(X_train, y_train, epochs=500, batch_size=100, validation_data=(X_test, y_test), verbose = 0)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nAll the 270 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n270 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 436, in inner_f\n    return f(**kwargs)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 711, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 236, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 725, in <lambda>\n    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 436, in inner_f\n    return f(**kwargs)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 552, in __init__\n    self.set_info(\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 436, in inner_f\n    return f(**kwargs)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 592, in set_info\n    self.set_label(label)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 723, in set_label\n    dispatch_meta_backend(self, label, 'label', 'float')\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py\", line 695, in dispatch_meta_backend\n    _validate_meta_shape(data)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py\", line 625, in _validate_meta_shape\n    assert len(data.shape) == 1 or (\nAssertionError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39m# Perform grid search with cross-validation\u001b[39;00m\n\u001b[0;32m     32\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mmodel, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39mcv, scoring\u001b[39m=\u001b[39mscoring, refit\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X_train, y_train) \u001b[39m#, sample_weight=feature_weights)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m# Print the best hyperparameters and corresponding performance metrics\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest hyperparameters:\u001b[39m\u001b[39m'\u001b[39m, grid_search\u001b[39m.\u001b[39mbest_params_)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[0;32m    845\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    846\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[0;32m    849\u001b[0m     )\n\u001b[1;32m--> 851\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[0;32m    853\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 270 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n270 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 436, in inner_f\n    return f(**kwargs)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 711, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 236, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 725, in <lambda>\n    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 436, in inner_f\n    return f(**kwargs)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 552, in __init__\n    self.set_info(\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 436, in inner_f\n    return f(**kwargs)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 592, in set_info\n    self.set_label(label)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 723, in set_label\n    dispatch_meta_backend(self, label, 'label', 'float')\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py\", line 695, in dispatch_meta_backend\n    _validate_meta_shape(data)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py\", line 625, in _validate_meta_shape\n    assert len(data.shape) == 1 or (\nAssertionError\n"
     ]
    }
   ],
   "source": [
    "X, y = df1[df1[\"Time\"] == \"Initial\"].drop([\"numVol\", \"Time\"], axis=1), df1[df1[\"Time\"] == \"Final\"].drop(['Time','numVol', 'Sweetener', 'Time', 'Sex', 'Weight','BMI', 'Fat', 'CVRI', 'Bpmin', 'Bpmax', 'Frec'], axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Define custom weights for input features\n",
    "#feature_weights = np.array([0.2, 0.5, 0.8, 1.0, 0.7, 0.5, 0.6, 0.9, 0.8, 0.5, \n",
    "#                            0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 0.6, 0.4, 0.3, 0.5, \n",
    "#                            0.8, 0.9, 1.0, 0.8, 0.7, 0.5, 0.4, 0.3, 0.2, 0.1])\n",
    "\n",
    "# Initialize XGBoost model with multioutput regression objective\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = {'n_estimators': [100, 500, 1000],\n",
    "              'learning_rate': [0.01, 0.1, 0.5],\n",
    "              'max_depth': [3, 5, 7]}\n",
    "\n",
    "# Define the cross-validation scheme\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the scoring metrics to use\n",
    "scoring = {'mse': 'neg_mean_squared_error',\n",
    "           'mae': 'neg_mean_absolute_error',\n",
    "           'r2': 'r2'}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring=scoring, refit='mse')\n",
    "grid_search.fit(X_train, y_train) #, sample_weight=feature_weights)\n",
    "\n",
    "# Print the best hyperparameters and corresponding performance metrics\n",
    "print('Best hyperparameters:', grid_search.best_params_)\n",
    "print('Best mean squared error:', -grid_search.best_score_)\n",
    "print('Best mean absolute error:', -grid_search.cv_results_['mean_test_mae'][grid_search.best_index_])\n",
    "print('Best R-squared:', grid_search.cv_results_['mean_test_r2'][grid_search.best_index_])\n",
    "\n",
    "# Use the best model to simulate an interventional study\n",
    "# Assume you have loaded the test data into a separate array called X_test\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# y_pred is a 2D array with 21 columns, one for each response variable\n",
    "# Each row corresponds to a single patient in the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 125 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n125 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 436, in inner_f\n    return f(**kwargs)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 711, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 236, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 725, in <lambda>\n    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 436, in inner_f\n    return f(**kwargs)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 552, in __init__\n    self.set_info(\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 436, in inner_f\n    return f(**kwargs)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 592, in set_info\n    self.set_label(label)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 723, in set_label\n    dispatch_meta_backend(self, label, 'label', 'float')\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py\", line 695, in dispatch_meta_backend\n    _validate_meta_shape(data)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py\", line 625, in _validate_meta_shape\n    assert len(data.shape) == 1 or (\nAssertionError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 18\u001b[0m\n\u001b[0;32m     10\u001b[0m xgbr \u001b[39m=\u001b[39m xgb\u001b[39m.\u001b[39mXGBRegressor(seed \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m)\n\u001b[0;32m     12\u001b[0m clf \u001b[39m=\u001b[39m RandomizedSearchCV(estimator\u001b[39m=\u001b[39mxgbr,\n\u001b[0;32m     13\u001b[0m                          param_distributions\u001b[39m=\u001b[39mparams,\n\u001b[0;32m     14\u001b[0m                          scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mneg_mean_squared_error\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m                          n_iter\u001b[39m=\u001b[39m\u001b[39m25\u001b[39m,\n\u001b[0;32m     16\u001b[0m                          verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m clf\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest parameters:\u001b[39m\u001b[39m\"\u001b[39m, clf\u001b[39m.\u001b[39mbest_params_)\n\u001b[0;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLowest RMSE: \u001b[39m\u001b[39m\"\u001b[39m, (\u001b[39m-\u001b[39mclf\u001b[39m.\u001b[39mbest_score_)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2.0\u001b[39m))\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1768\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1766\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1767\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1768\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1769\u001b[0m         ParameterSampler(\n\u001b[0;32m   1770\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1771\u001b[0m         )\n\u001b[0;32m   1772\u001b[0m     )\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:851\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[0;32m    845\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    846\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    848\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[0;32m    849\u001b[0m     )\n\u001b[1;32m--> 851\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[0;32m    853\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    854\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    855\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[0;32m    361\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    369\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    377\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 125 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n125 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 436, in inner_f\n    return f(**kwargs)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 711, in fit\n    train_dmatrix, evals = _wrap_evaluation_matrices(\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 236, in _wrap_evaluation_matrices\n    train_dmatrix = create_dmatrix(\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\", line 725, in <lambda>\n    create_dmatrix=lambda **kwargs: DMatrix(nthread=self.n_jobs, **kwargs),\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 436, in inner_f\n    return f(**kwargs)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 552, in __init__\n    self.set_info(\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 436, in inner_f\n    return f(**kwargs)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 592, in set_info\n    self.set_label(label)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\", line 723, in set_label\n    dispatch_meta_backend(self, label, 'label', 'float')\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py\", line 695, in dispatch_meta_backend\n    _validate_meta_shape(data)\n  File \"c:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\data.py\", line 625, in _validate_meta_shape\n    assert len(data.shape) == 1 or (\nAssertionError\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold, RandomizedSearchCV\n",
    "\n",
    "params = { 'max_depth': [3, 5, 6, 10, 15, 20],\n",
    "           'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "           'subsample': np.arange(0.5, 1.0, 0.1),\n",
    "           'colsample_bytree': np.arange(0.4, 1.0, 0.1),\n",
    "           'colsample_bylevel': np.arange(0.4, 1.0, 0.1),\n",
    "           'n_estimators': [100, 500, 1000]}\n",
    "\n",
    "xgbr = xgb.XGBRegressor(seed = 20)\n",
    "\n",
    "clf = RandomizedSearchCV(estimator=xgbr,\n",
    "                         param_distributions=params,\n",
    "                         scoring='neg_mean_squared_error',\n",
    "                         n_iter=25,\n",
    "                         verbose=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", clf.best_params_)\n",
    "print(\"Lowest RMSE: \", (-clf.best_score_)**(1/2.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
